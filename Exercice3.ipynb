{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43de0682-4f52-40bf-af2e-8dd499d091b3",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f2288-e6d4-475d-8505-abdc95c2deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"\\nWord2Vec\")\n",
    "phrase_entrainement = [\n",
    "    [\"I\", \"prefer\", \"eating\", \"burgers\", \"at\",\"the\", \"restaurant\"],\n",
    "    [\"The\", \"cat\", \"sleeps\", \"on\", \"floor\"],\n",
    "    [\"We\", \"like\", \"fast\", \"food\", \"and\", \"good\",\"drinks\"],\n",
    "    [\"Would\", \"you\", \"like\", \"some\", \"coffee\", \"or\", \"tea\"],\n",
    "]\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=phrase_entrainement, vector_size=8, window=2, min_count=1, epochs=10, seed=42)\n",
    "\n",
    "#Phrase\n",
    "phrase = [\"I\", \"prefer\", \"eating\", \"some\", \"good\", \"burgers\", \"at\", \"the\", \"restaurant\"]\n",
    "print(f\"\\nPhrase: {' '.join(phrase)}\")\n",
    "\n",
    "\n",
    "inputs = np.array([model.wv[word] for word in phrase])\n",
    "max_len = len(phrase)\n",
    "\n",
    "\n",
    "#initialisation xavier\n",
    "def xavier_initialization(shape):\n",
    "    limit = np.sqrt(6 / (shape[0] + shape[1]))\n",
    "    return np.random.uniform(-limit, limit, shape)\n",
    "\n",
    "\n",
    "dimension = model.vector_size\n",
    "np.random.seed(42)\n",
    "Wq_norm = np.random.randn(dimension, dimension) \n",
    "Wk_norm = np.random.randn(dimension, dimension) \n",
    "Wv_norm = np.random.randn(dimension, dimension) \n",
    "\n",
    "Wq_xav = xavier_initialization((dimension, dimension))\n",
    "Wk_xav = xavier_initialization((dimension, dimension))\n",
    "Wv_xav = xavier_initialization((dimension, dimension))\n",
    "\n",
    "#Positional encoding\n",
    "def positional_encoding(max_len, d, n=10000):\n",
    "    P = np.zeros((max_len, d))\n",
    "    for k in range(max_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n",
    "\n",
    "\n",
    "pos_enc = positional_encoding(max_len, dimension)\n",
    "inputs_pos = inputs + pos_enc\n",
    "\n",
    "\n",
    "def calcul_Q_K_V(X, Wq, Wk, Wv):\n",
    "    Q = np.dot(X, Wq)\n",
    "    K = np.dot(X, Wk)\n",
    "    V = np.dot(X, Wv)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n",
    "    weights = softmax(scores)\n",
    "    output = np.dot(weights, V)\n",
    "    return scores, weights, output\n",
    "\n",
    "#Normal \n",
    "scores1, weight1, output1 = calcul_Q_K_V(inputs, Wq_norm, Wk_norm, Wv_norm)\n",
    "\n",
    "#Xavier \n",
    "scores2, weight2, output2 = calcul_Q_K_V(inputs, Wq_xav, Wk_xav, Wv_xav)\n",
    "\n",
    "# Xavier + Positional Encoding\n",
    "scores3, weight3, output3 = calcul_Q_K_V(inputs_pos, Wq_xav, Wk_xav, Wv_xav)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(weight1, annot=True, cmap=\"viridis\", xticklabels=phrase, yticklabels=phrase)\n",
    "plt.title(\"Attention Weights Initialisation Normale\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.heatmap(weight2, annot=True, cmap=\"viridis\", xticklabels=phrase, yticklabels=phrase)\n",
    "plt.title(\"Attention Weights Initialisation Xavier\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.heatmap(weight1 - weight2, annot=True, cmap=\"hot\", xticklabels=phrase, yticklabels=phrase)\n",
    "plt.title(\"Difference Xavier vs Normal\")\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.heatmap(pos_enc, annot=True, cmap=\"viridis\", yticklabels=phrase)\n",
    "plt.title(\"Encodage Positionnel \")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.heatmap(weight3, annot=True, cmap=\"viridis\", xticklabels=phrase, yticklabels=phrase)\n",
    "plt.title(\"Attention Weights - Xavier + Position\")\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.heatmap(weight3-weight2, annot=True, cmap=\"hot\", xticklabels=phrase, yticklabels=phrase)\n",
    "plt.title(\"Différence avec/sans Position\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f56b3-5093-41f5-b703-52ee1929e80f",
   "metadata": {},
   "source": [
    "1- L'initialisation des poids agit sur la stabilité et la qualité de l'attention. Donc si l'initialisation est aléatoire,  les poids peuvent donner une attention qui est moins bonne, Xavier permet de contourner ca\n",
    "\n",
    "\n",
    "\n",
    "2- L'encodage positionnel ajoute une information sur la position de chaque mot, ce qui permet au modele de differencier des phrases comme 'le chat dort' et 'dort le chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec5171-33d7-4522-816a-851508d2a32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
