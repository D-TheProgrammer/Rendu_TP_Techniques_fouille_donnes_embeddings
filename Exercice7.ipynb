{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8242ba7a-b085-49a3-b601-7fd820176afa",
   "metadata": {},
   "source": [
    "# Exercice 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14195bdd-fca6-4350-ad0c-fc3c37746ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def calcul_attention(phrase_texte):\n",
    "   \n",
    "    mots = phrase_texte.split()\n",
    "    if len(mots) == 0:\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    vocab_size = len(mots)\n",
    "    embeddings = np.eye(vocab_size)\n",
    "    \n",
    "    np.random.seed(42)  \n",
    "    Wq = np.random.randn(vocab_size, vocab_size)\n",
    "    Wk = np.random.randn(vocab_size, vocab_size)\n",
    "    Wv = np.random.randn(vocab_size, vocab_size)\n",
    "\n",
    "\n",
    "    Q = np.dot(embeddings, Wq)\n",
    "    K = np.dot(embeddings, Wk)\n",
    "    V = np.dot(embeddings, Wv)\n",
    "    \n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    return mots, attention_weights\n",
    "\n",
    "def visualisation_interactive(phrase_texte):\n",
    "\n",
    "    if not phrase_texte.strip():\n",
    "        return\n",
    "    \n",
    "    result = calcul_attention(phrase_texte)\n",
    "    if result is None:\n",
    "        print(\"Phrase vide!\")\n",
    "        return\n",
    "    else:\n",
    "        mots, attention = result\n",
    "    \n",
    "    #heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention, annot=True, cmap=\"viridis\", \n",
    "                xticklabels=mots, yticklabels=mots, fmt='.3f')\n",
    "    plt.title(f\"Self-Attention: '{phrase_texte}'\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #Analyse\n",
    "    print(\"Analyse: \",phrase_texte)\n",
    "    for i, mot in enumerate(mots):\n",
    "        attention_sans_lui_meme= attention[i].copy()\n",
    "        #ou le mot est on met 0 pour ne pas avoir bank = bank\n",
    "        attention_sans_lui_meme[i]= 0  \n",
    "        \n",
    "        index_max = np.argmax(attention_sans_lui_meme)\n",
    "        if attention_sans_lui_meme[index_max] > 0.1:\n",
    "            print(f\"   '{mot}' ->  '{mots[index_max]}' (score: {attention_sans_lui_meme[index_max]:.3f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"OUTIL INTERACTIF pour Self-Attention mechanism\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "while True:\n",
    "    phrase = input(\"\\nEntrez une phrase (ou 'quitter' ou 'q' pour quitter): \")\n",
    "    \n",
    "    if phrase.lower() == 'quitter' or phrase.lower() == 'q' :\n",
    "        print(\"Au revoir !\")\n",
    "        break\n",
    "        \n",
    "    visualisation_interactive(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce4930-d1c6-4000-9a80-08e8e385b3e9",
   "metadata": {},
   "source": [
    "Reponse:\n",
    "Dans ce code, les poids d'attention sont générés aléatoirement etdonc ils et ne font pas comme un vrai apprentissage linguistique\n",
    "\n",
    "Cependant dans un modèle ENTRAINE (comme BERT) , ca aurait compris que 'bank' 'money' et 'lend' sont dans un contexte financier mais que \n",
    "'bank' , 'river', 'water' dans un contexte géographique \n",
    "Ca comprendrait vraiment les vraies sens et relation des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df3097-c2c1-4ec7-bc34-b6dac271ba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025985c-502e-4853-a236-6b66ee177bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
