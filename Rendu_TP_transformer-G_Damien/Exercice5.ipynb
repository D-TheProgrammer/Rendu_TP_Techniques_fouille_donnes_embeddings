{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f266f37-cbb4-4432-bd69-078b92313e86",
   "metadata": {},
   "source": [
    "# Exercice 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb79344-c9bd-48e3-8a0e-dca24573dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def layer_norm(x):\n",
    "    return (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"I am very happy to work at Paris and love Marseille university\".split()\n",
    "vocab_size = len(sentence)\n",
    "\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(sentence)}\n",
    "embeddings = np.eye(vocab_size)\n",
    "inputs = np.array([embeddings[word_to_index[word]] for word in sentence])\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "Wq = np.random.rand(vocab_size, vocab_size)\n",
    "Wk = np.random.rand(vocab_size, vocab_size)\n",
    "Wv = np.random.rand(vocab_size, vocab_size)\n",
    "W1 = np.random.rand(vocab_size, vocab_size)\n",
    "W2 = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "\n",
    "#sans Layer normalisation\n",
    "Q = np.dot(inputs, Wq)\n",
    "K = np.dot(inputs, Wk)\n",
    "V = np.dot(inputs, Wv)\n",
    "scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "attention_weights_sans_norm = softmax(scores)\n",
    "attention_output_sans_norm = np.dot(attention_weights_sans_norm, V)\n",
    "\n",
    "#Layer normalisation AVANT attention\n",
    "inputs_norm = layer_norm(inputs)\n",
    "Q_norm_Avant = np.dot(inputs_norm, Wq)\n",
    "K_norm_Avant = np.dot(inputs_norm, Wk)\n",
    "V_norm_Avant = np.dot(inputs_norm, Wv)\n",
    "scores_norm_Avant = np.dot(Q_norm_Avant, K_norm_Avant.T) / np.sqrt(K_norm_Avant.shape[1])\n",
    "attention_weights_norm_Avant = softmax(scores_norm_Avant)\n",
    "attention_output_norm_Avant = np.dot(attention_weights_norm_Avant, V_norm_Avant)\n",
    "\n",
    "#Layer normalisation APRES attention\n",
    "attention_output_norm_Apres = layer_norm(attention_output_sans_norm + inputs) \n",
    "\n",
    "\n",
    "\n",
    "#Feed-Forward Layer relu\n",
    "ffn_relu =np.dot(attention_output_sans_norm, W1)\n",
    "ffn_relu= np.maximum(0, ffn_relu)\n",
    "ffn_relu= np.dot(ffn_relu, W2)\n",
    "encoder_output_relu = layer_norm(ffn_relu + attention_output_sans_norm)\n",
    "\n",
    "\n",
    "def leaky_relu(x):\n",
    "    alpha=0.01\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "def elu(x):\n",
    "    alpha=1\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "\n",
    "\n",
    "#leaky ReLU\n",
    "ffn_leaky =np.dot(attention_output_sans_norm, W1)\n",
    "ffn_leaky= leaky_relu(ffn_leaky)\n",
    "ffn_leaky= np.dot(ffn_leaky, W2)\n",
    "encoder_output_leaky = layer_norm(ffn_leaky + attention_output_sans_norm)\n",
    "\n",
    "#elu\n",
    "ffn_elu =np.dot(attention_output_sans_norm, W1)\n",
    "ffn_elu= elu(ffn_elu)\n",
    "ffn_elu= np.dot(ffn_elu, W2)\n",
    "encoder_output_elu = layer_norm(ffn_elu + attention_output_sans_norm) \n",
    "\n",
    "#masquage\n",
    "#difference phrases de longueurs diffrentes\n",
    "phrases = [\n",
    "    [\"I\", \"love\", \"Marseille\"],\n",
    "    [\"I\", \"am\", \"very\", \"happy\", \"to\", \"work\", \"at\", \"Paris\"]\n",
    "]\n",
    "\n",
    "max_len = max(len(s) for s in phrases)\n",
    "\n",
    "#padding\n",
    "liste_inputs = []\n",
    "for ligne in phrases:\n",
    "    emb = [embeddings[word_to_index[word]] for word in ligne]\n",
    "    #padding selon longueur phrase\n",
    "    while len(emb) < max_len:\n",
    "        emb.append(np.zeros(vocab_size))  #des zeros de taille vocab_size\n",
    "    liste_inputs.append(np.array(emb))\n",
    "\n",
    "inputs_padded = np.array(liste_inputs)\n",
    "\n",
    "#Q,K,V masquer\n",
    "np.random.seed(0)\n",
    "Wq_mask = np.random.rand(vocab_size, vocab_size)\n",
    "Wk_mask = np.random.rand(vocab_size, vocab_size)\n",
    "Wv_mask = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "#ensemble de resultat\n",
    "resulats_ensemble_mask = []\n",
    "\n",
    "for i, phrase_inputs in enumerate(inputs_padded): \n",
    "    \n",
    "    vrai_longueur = len(phrases[i])\n",
    "    \n",
    "    Q =np.dot(phrase_inputs, Wq_mask)\n",
    "    K =np.dot(phrase_inputs, Wk_mask)\n",
    "    V =np.dot(phrase_inputs, Wv_mask)\n",
    "\n",
    "\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "\n",
    "    scores_masquer= scores.copy()\n",
    "    scores_masquer[:, vrai_longueur:]= -1000000000000 \n",
    "    scores_masquer[vrai_longueur:, :]= -1000000000000\n",
    "    \n",
    "    attention_weights_sans_mask = softmax(scores)\n",
    "    attention_weights_mask = softmax(scores_masquer)\n",
    "    \n",
    "    #pour empecher nan\n",
    "    attention_weights_mask[vrai_longueur:, :] = 0.0\n",
    "      \n",
    "    output_sans_mask = np.dot(attention_weights_sans_mask, V)\n",
    "    output_mask = np.dot(attention_weights_mask, V)\n",
    "    \n",
    "    phrase_label = phrases[i] + ['padding'] * (max_len - vrai_longueur)\n",
    "    \n",
    "    resulats_ensemble_mask.append({\n",
    "        'phrase': phrases[i],\n",
    "        'phrase_label': phrase_label,\n",
    "        'vrai_longueur': vrai_longueur,\n",
    "        'attention_weights_sans_mask': attention_weights_sans_mask,\n",
    "        'attention_weights_mask': attention_weights_mask,\n",
    "        'output_sans_mask': output_sans_mask,\n",
    "        'output_mask': output_mask\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "#layer normalisation\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(attention_weights_sans_norm, annot=True, cmap='viridis', xticklabels=sentence, yticklabels=sentence, fmt='.2f')\n",
    "plt.title('Sans Layer Normalisation')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.heatmap(attention_weights_norm_Avant, annot=True, cmap='viridis', xticklabels=sentence, yticklabels=sentence, fmt='.2f')\n",
    "plt.title('Layer Normalisation AVANT')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.heatmap(attention_weights_norm_Avant - attention_weights_sans_norm, annot=True, cmap='hot', xticklabels=sentence, yticklabels=sentence, fmt='.2f')\n",
    "plt.title('Différence Normalisation AVANT vs Sans')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.heatmap(attention_output_sans_norm, annot=True, cmap='viridis', yticklabels=sentence, fmt='.2f')\n",
    "plt.title('Output Attention Sans Normalisation')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.heatmap(attention_output_norm_Apres, annot=True, cmap='viridis', yticklabels=sentence, fmt='.2f')\n",
    "plt.title('Output Attention Norm APRÈS')\n",
    "\n",
    "plt.suptitle('Layer Normalization', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#activation\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.heatmap(encoder_output_relu, annot=True, cmap='viridis', yticklabels=sentence)\n",
    "plt.title('FFN Output ReLU (avec entrées positives)')\n",
    "\n",
    "plt.subplot(2,2 ,2)\n",
    "sns.heatmap(encoder_output_leaky, annot=True, cmap='viridis', yticklabels=sentence)\n",
    "plt.title('FFN Output Leaky ReLU (identique car x > 0)')\n",
    "\n",
    "plt.subplot(2,2 ,3)\n",
    "sns.heatmap(encoder_output_elu, annot=True, cmap='viridis', yticklabels=sentence)\n",
    "plt.title('FFN Output ELU (identique car x > 0)')\n",
    "\n",
    "plt.subplot(2,2 ,4)\n",
    "diff_activations = np.abs(encoder_output_leaky - encoder_output_relu)\n",
    "sns.heatmap(diff_activations, annot=True, cmap='hot', yticklabels=sentence)\n",
    "plt.title('Différence Leaky ReLU vs ReLU ( 0 car x positifs)')\n",
    "\n",
    "plt.suptitle('Fonctions activation entrees positives donne résultats similaires', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#mask\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "for i, result in enumerate(resulats_ensemble_mask):\n",
    "    #sans masque\n",
    "    plt.subplot(2, 2, i*2 + 1)\n",
    "    sns.heatmap(result['attention_weights_sans_mask'], annot=True, cmap='viridis', xticklabels=result['phrase_label'], yticklabels=result['phrase_label'])\n",
    "    plt.title(f'Phrase {i+1} SANS Masque)')\n",
    "    \n",
    "    # Avec masque\n",
    "    plt.subplot(2, 2, i*2 + 2)\n",
    "    sns.heatmap(result['attention_weights_mask'], annot=True, cmap='viridis', xticklabels=result['phrase_label'], yticklabels=result['phrase_label'])\n",
    "    plt.title(f'Phrase {i+1} AVEC Masque')\n",
    "\n",
    "plt.suptitle('Mask', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb3ad4-9d97-42cb-bb14-efc1c7ca031d",
   "metadata": {},
   "source": [
    "1- La normalisation stabilise l'attention pour cela elle stabilise les échelles des données. \n",
    "Avant l'attention, elle équilibre la distribution des poids. \n",
    "Après elle standardise les sorties pour les couches suivantes. \n",
    "La position change les résultats mais améliore quand même la stabilité\n",
    "\n",
    "\n",
    "2- Avec masquage, l'attention ignore  le padding. \n",
    "Sans masquage, elle gaspille du poids sur les tokens vides. \n",
    "Le masquage préserve le sens en concentrant l'attention uniquement sur les mots réels, \n",
    "c'est essentiel pour traiter des phrases de longueurs différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40f25a-d6f7-4511-a1fa-4035951a4572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
