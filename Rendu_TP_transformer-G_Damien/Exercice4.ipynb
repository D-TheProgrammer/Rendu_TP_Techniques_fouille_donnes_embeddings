{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f489887-aeb3-43e0-89dd-220bf760c2f1",
   "metadata": {},
   "source": [
    "# Exercice 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557e8c7-3607-41fe-9c06-882da4a7c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def dropout(x, taille=0.1):\n",
    "    if taille == 0:\n",
    "        return x\n",
    "    mask = np.random.binomial(1, 1-taille, size=x.shape) / (1-taille)\n",
    "    return x * mask\n",
    "\n",
    "\n",
    "def multi_head_attention(Q, K, V, num_heads, taille_drop=0.0):\n",
    "    d_model = Q.shape[-1] \n",
    "    d_k = d_model // num_heads \n",
    "    \n",
    "    Q_heads = [np.dot(Q, np.random.rand(d_model, d_k)) for _ in range(num_heads)]\n",
    "    K_heads = [np.dot(K, np.random.rand(d_model, d_k)) for _ in range(num_heads)]\n",
    "    V_heads = [np.dot(V, np.random.rand(d_model, d_k)) for _ in range(num_heads)]\n",
    "    \n",
    "    liste_attention_outputs = []\n",
    "    liste_attention_weights = []\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        scores = np.dot(Q_heads[i], K_heads[i].T) / np.sqrt(d_k)\n",
    "        attention_weights = softmax(scores)\n",
    "        attention_weights = dropout(attention_weights, taille_drop)\n",
    "        attention_output = np.dot(attention_weights, V_heads[i])\n",
    "        liste_attention_outputs.append(attention_output)\n",
    "        liste_attention_weights.append(attention_weights)\n",
    "    \n",
    "    concatenated_output = np.concatenate(liste_attention_outputs, axis=-1)\n",
    "    output = np.dot(concatenated_output, np.random.rand(concatenated_output.shape[-1], d_model))\n",
    "    return output, liste_attention_weights\n",
    "\n",
    "\n",
    "def single_head_attention(Q, K, V, taille_drop=0.0):\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "    attention_weights = softmax(scores)\n",
    "    attention_weights = dropout(attention_weights, taille_drop)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"I am very happy to work at Paris 8 university\".split()\n",
    "vocab_size = len(sentence)\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(sentence)}\n",
    "embeddings = np.eye(vocab_size)\n",
    "inputs = np.array([embeddings[word_to_index[word]] for word in sentence])\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "print(\"Changement de dimension\")\n",
    "dimension_embed = [4, 8, 16]\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for indice_dimension, dimension in enumerate(dimension_embed):\n",
    "    print(\"\\n dimension: \",dimension)\n",
    "    \n",
    "    prj = np.random.rand(vocab_size, dimension)\n",
    "    inputs_p = np.dot(inputs, prj)\n",
    "    \n",
    "\n",
    "    Wq = np.random.rand(dimension, dimension)\n",
    "    Wk = np.random.rand(dimension, dimension)\n",
    "    Wv = np.random.rand(dimension, dimension)\n",
    "    \n",
    "    Q = np.dot(inputs_p, Wq)\n",
    "    K = np.dot(inputs_p, Wk)\n",
    "    V = np.dot(inputs_p, Wv)\n",
    "    \n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "    attention_weights = softmax(scores)\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    plt.subplot(3, 3, indice_dimension*3 + 1)\n",
    "    sns.heatmap(Q, annot=True, fmt='.2f', cmap='viridis', xticklabels=range(dimension), yticklabels=sentence)\n",
    "    plt.title(f'Query ({dimension})')\n",
    "    \n",
    "    plt.subplot(3, 3, indice_dimension*3 + 2)\n",
    "    sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "    plt.title(f'Attention Weights ({dimension})')\n",
    "    \n",
    "    plt.subplot(3, 3, indice_dimension*3 + 3)\n",
    "    sns.heatmap(output, annot=True, fmt='.2f', cmap='viridis', xticklabels=range(dimension), yticklabels=sentence)\n",
    "    plt.title(f'Output ({dimension})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nMulti-Head Attention ===\")\n",
    "\n",
    "\n",
    "dimension = 8\n",
    "prj = np.random.rand(vocab_size, dimension)\n",
    "inputs_p = np.dot(inputs, prj)\n",
    "\n",
    "Wq = np.random.rand(dimension, dimension)\n",
    "Wk = np.random.rand(dimension, dimension)\n",
    "Wv = np.random.rand(dimension, dimension)\n",
    "\n",
    "Q = np.dot(inputs_p, Wq)\n",
    "K = np.dot(inputs_p, Wk)\n",
    "V = np.dot(inputs_p, Wv)\n",
    "\n",
    "liste_num_head = [1, 2, 4]\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for indice_tete, num_heads in enumerate(liste_num_head):\n",
    "    print(\"\\n nombre tete: \",num_heads)\n",
    "    \n",
    "    if num_heads == 1:\n",
    "        output, attention_weights = single_head_attention(Q, K, V)\n",
    "    else:\n",
    "        output, liste_attention_weights = multi_head_attention(Q, K, V, num_heads)\n",
    "    \n",
    "    plt.subplot(3, 4, indice_tete*4 + 1)\n",
    "    sns.heatmap(Q, annot=True, fmt='.2f', cmap='viridis', xticklabels=range(dimension), yticklabels=sentence)\n",
    "    plt.title(f'Query (Q) ({num_heads})')\n",
    "    \n",
    "    plt.subplot(3, 4, indice_tete*4 + 2)\n",
    "    if num_heads == 1:\n",
    "        sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "    else:\n",
    "        sns.heatmap(liste_attention_weights[0], annot=True, fmt='.2f', cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "    plt.title(f'Attention Weights ({num_heads})')\n",
    "    \n",
    "    plt.subplot(3, 4, indice_tete*4 + 3)\n",
    "    sns.heatmap(output, annot=True, fmt='.2f', cmap='viridis', xticklabels=range(dimension), yticklabels=sentence)\n",
    "    plt.title(f'Output ({num_heads})')\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDropout\")\n",
    "\n",
    "ensemble_drops = [0.0, 0.3]\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for indice_drop, taille_drop in enumerate(ensemble_drops):\n",
    "    print(\"\\nDropout : \",taille_drop)\n",
    "    \n",
    "    output, attention_weights = single_head_attention(Q, K, V, taille_drop)\n",
    "    \n",
    "    plt.subplot(2, 2, indice_drop*2 + 1)\n",
    "    sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "    plt.title(f'Attention Weights (Dropout={taille_drop})')\n",
    "    \n",
    "    plt.subplot(2, 2, indice_drop*2 + 2)\n",
    "    sns.heatmap(output, annot=True, fmt='.2f', cmap='viridis', \n",
    "                xticklabels=range(dimension), yticklabels=sentence)\n",
    "    plt.title(f'Output (Dropout={taille_drop})')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
