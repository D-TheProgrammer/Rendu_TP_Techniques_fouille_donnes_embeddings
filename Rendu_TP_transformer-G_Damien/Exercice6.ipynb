{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180bc2f9-f66a-456f-bcef-e2146a82cc06",
   "metadata": {},
   "source": [
    "# Exercice 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b302f5-cd33-44a8-9964-18ae275590fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def layer_norm(x):\n",
    "    return (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"Barack Obama was the 44th president of the United States\".split()\n",
    "vocab_size = len(sentence)\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(sentence)}\n",
    "embeddings = np.eye(vocab_size)\n",
    "\n",
    "inputs = np.array([embeddings[word_to_index[word]] for word in sentence])\n",
    "\n",
    "np.random.seed(42)\n",
    "Wq = np.random.rand(vocab_size, vocab_size)\n",
    "Wk = np.random.rand(vocab_size, vocab_size)\n",
    "Wv = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "\n",
    "Q = np.dot(inputs, Wq)\n",
    "K = np.dot(inputs, Wk)\n",
    "V = np.dot(inputs, Wv)\n",
    "\n",
    "scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "attention_weights = softmax(scores)\n",
    "output = np.dot(attention_weights, V)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(inputs, annot=True, cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "plt.title('Inputs pour entités Nommées')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.heatmap(attention_weights, annot=True, cmap='viridis', xticklabels=sentence, yticklabels=sentence)\n",
    "plt.title('Attention Weights pour entités Nommées')\n",
    "\n",
    "\n",
    "print(\"\\nAnalyse:\")\n",
    "\n",
    "for i, mot in enumerate(sentence):\n",
    "    attention_sans_lui_meme= attention_weights[i].copy()\n",
    "    #ou le mot est on met 0 pour ne pas avoir bank = bank\n",
    "    attention_sans_lui_meme[i]= 0  \n",
    "    \n",
    "    index_max = np.argmax(attention_sans_lui_meme)\n",
    "    if attention_sans_lui_meme[index_max] > 0.1:\n",
    "        print(f\"   '{mot}' ->  '{sentence[index_max]}' (score: {attention_sans_lui_meme[index_max]:.3f})\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Attention traduction sequence sequence\")\n",
    "encoder_sentence = \"i am the president\".split()\n",
    "decoder_sentence = \"je suis le president \".split()\n",
    "\n",
    "encoder_vocab_size = len(encoder_sentence)\n",
    "decoder_vocab_size = len(decoder_sentence)\n",
    "\n",
    "\n",
    "encoder_word_to_index = {word: idx for idx, word in enumerate(encoder_sentence)}\n",
    "decoder_word_to_index = {word: idx for idx, word in enumerate(decoder_sentence)}\n",
    "encoder_embeddings = np.eye(encoder_vocab_size)\n",
    "decoder_embeddings = np.eye(decoder_vocab_size)\n",
    "\n",
    "encoder_inputs = np.array([encoder_embeddings[encoder_word_to_index[word]] for word in encoder_sentence])\n",
    "decoder_inputs = np.array([decoder_embeddings[decoder_word_to_index[word]] for word in decoder_sentence])\n",
    "\n",
    "np.random.seed(42)\n",
    "Wq_enc = np.random.rand(encoder_vocab_size, encoder_vocab_size)\n",
    "Wk_enc = np.random.rand(encoder_vocab_size, encoder_vocab_size)\n",
    "Wv_enc = np.random.rand(encoder_vocab_size, encoder_vocab_size)\n",
    "\n",
    "Wq_dec = np.random.rand(decoder_vocab_size, decoder_vocab_size)\n",
    "Wk_dec = np.random.rand(decoder_vocab_size, decoder_vocab_size)\n",
    "Wv_dec = np.random.rand(decoder_vocab_size, decoder_vocab_size)\n",
    "\n",
    "#encodeur\n",
    "Q_enc = np.dot(encoder_inputs, Wq_enc)\n",
    "K_enc = np.dot(encoder_inputs, Wk_enc)\n",
    "V_enc = np.dot(encoder_inputs, Wv_enc)\n",
    "scores_enc = np.dot(Q_enc, K_enc.T) / np.sqrt(K_enc.shape[1])\n",
    "weights_enc = softmax(scores_enc)\n",
    "output_enc = np.dot(weights_enc, V_enc)\n",
    "\n",
    "#décodeur\n",
    "Q_dec = np.dot(decoder_inputs, Wq_dec)\n",
    "K_dec = np.dot(decoder_inputs, Wk_dec)\n",
    "V_dec = np.dot(decoder_inputs, Wv_dec)\n",
    "scores_dec = np.dot(Q_dec, K_dec.T) / np.sqrt(K_dec.shape[1])\n",
    "weights_dec = softmax(scores_dec)\n",
    "output_dec = np.dot(weights_dec, V_dec)\n",
    "\n",
    "\n",
    "#Visualisation traduction\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.heatmap(weights_enc, annot=True, cmap='viridis', xticklabels=encoder_sentence, yticklabels=encoder_sentence)\n",
    "plt.title('Encoder Attention Traduction')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.heatmap(weights_dec, annot=True, cmap='viridis', xticklabels=decoder_sentence, yticklabels=decoder_sentence)\n",
    "plt.title('Decoder attention traduction')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n attention pour classification de texte \")\n",
    "\n",
    "#sentences\n",
    "sentences = [\n",
    "    \"i love the all the citizen\",\n",
    "    \"the new law is even more unfair\"\n",
    "]\n",
    "\n",
    "\n",
    "classification_sentence = sentences[0].split()\n",
    "vocab_size_classification = len(classification_sentence)\n",
    "\n",
    "\n",
    "word_to_index_classification = {word: idx for idx, word in enumerate(classification_sentence)}\n",
    "embeddings_classification = np.eye(vocab_size_classification)\n",
    "inputs_classification = np.array([embeddings_classification[word_to_index_classification[word]] for word in classification_sentence])\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "Wq_classification = np.random.rand(vocab_size_classification, vocab_size_classification)\n",
    "Wk_classification = np.random.rand(vocab_size_classification, vocab_size_classification)\n",
    "Wv_classification = np.random.rand(vocab_size_classification, vocab_size_classification)\n",
    "\n",
    "Q_classification = np.dot(inputs_classification, Wq_classification)\n",
    "K_classification = np.dot(inputs_classification, Wk_classification)\n",
    "V_classification = np.dot(inputs_classification, Wv_classification)\n",
    "\n",
    "scores_classification = np.dot(Q_classification, K_classification.T) / np.sqrt(K_classification.shape[1])\n",
    "weights_classification = softmax(scores_classification)\n",
    "output_classification = np.dot(weights_classification, V_classification)\n",
    "\n",
    "\n",
    "\n",
    "attention_classifcaition = np.mean(output_classification, axis=0)\n",
    "\n",
    "\n",
    "scores_relation = np.dot(Q_dec, K_enc.T) / np.sqrt(K_enc.shape[1])\n",
    "weights_relation = softmax(scores_relation)\n",
    "output_relation = np.dot(weights_relation, V_enc)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.heatmap(weights_relation, annot=True, cmap='viridis',\n",
    "            xticklabels=encoder_sentence, yticklabels=decoder_sentence)\n",
    "plt.title('Relation entre input et output sequence')\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.heatmap(weights_classification, annot=True, cmap='viridis', \n",
    "            xticklabels=classification_sentence, yticklabels=classification_sentence)\n",
    "plt.title(\"Classification attetion \")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nAnalyse classification:\")\n",
    "for i, mot in enumerate(classification_sentence):\n",
    "    attention_sans_lui_meme= weights_classification[i].copy()\n",
    "    #ou le mot est on met 0 pour ne pas avoir bank = bank\n",
    "    attention_sans_lui_meme[i]= 0  \n",
    "    \n",
    "    index_max = np.argmax(attention_sans_lui_meme)\n",
    "    if attention_sans_lui_meme[index_max] > 0.1:\n",
    "        print(f\"   '{mot}' ->  '{classification_sentence[index_max]}' (score: {attention_sans_lui_meme[index_max]:.3f})\")\n",
    "        \n",
    "\n",
    "\n",
    "print(f\"\\nFeatures pour classification: {attention_classifcaition}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ea7b4-75fd-4413-bb64-28cc4ab60167",
   "metadata": {},
   "source": [
    "            \n",
    "Le mécanisme d'attention relie 'Barack Obama' à 'United States',\n",
    "et 'president' a 'states' donc on voit bien le lien sémantiques \n",
    "\n",
    "Donc la self-attention gère bien les entités\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e4fd8-d2e7-4bac-83d3-fb0a5679591c",
   "metadata": {},
   "source": [
    "On peut alors comprendre son importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c02a98-d998-453c-93c2-5a812dc6dbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07339b-f806-4208-bcb7-e84155837a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
