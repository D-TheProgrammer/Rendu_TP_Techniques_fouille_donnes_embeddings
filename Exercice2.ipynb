{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31959fce-56be-485c-bb63-a08a58c27f4d",
   "metadata": {},
   "source": [
    "# Exercice 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f93642-6937-481e-93ce-a79ec5a56ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "# Example sentence embeddings (4 dimensions per word)\n",
    "embeddings = {\n",
    "    \"I\": [1, 0, 0, 0],\n",
    "    \"love\": [0, 1, 0, 0],\n",
    "    \"playing\": [0, 0, 1, 0],\n",
    "    \"tennis\": [0, 0, 0, 1],\n",
    "    \"football\": [1, 1, 0, 0],\n",
    "    \"often\": [1, 0, 1, 0],\n",
    "    \"with\": [0, 1, 1, 0],\n",
    "    \"friends\": [1, 1, 1, 0],\n",
    "    \"and\": [0, 0, 1, 1]\n",
    "}\n",
    "\n",
    "\n",
    "# Convert sentence to sequence of embeddings\n",
    "phrases = [\n",
    "    [\"I\", \"love\", \"playing\", \"football\", \"often\", \"with\", \"friends\"],\n",
    "    [\"I\", \"love\", \"tennis\"]\n",
    "]\n",
    "\n",
    "\n",
    "max_len = max(len(s) for s in phrases)\n",
    "\n",
    "liste_inputs = []\n",
    "for ligne in phrases:\n",
    "    emb = [embeddings[mot] for mot in ligne]\n",
    "    #padding selon longueur phrase\n",
    "    while len(emb) < max_len:\n",
    "        emb.append([0, 0, 0, 0])\n",
    "    liste_inputs.append(np.array(emb))\n",
    "    \n",
    "\n",
    "inputs = np.array(liste_inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize weights for Query, Key, and Value matrices\n",
    "np.random.seed(0)\n",
    "Wq = np.random.rand(4, 4)\n",
    "Wk = np.random.rand(4, 4)\n",
    "Wv = np.random.rand(4, 4)\n",
    "\n",
    "for i, phrase_inputs in enumerate(inputs): \n",
    "    print(\"\\n\\n------------------------------\")\n",
    "    print(f\"\\nANALYSE DE LA PHRASE {i+1}: {phrases[i]}\")\n",
    "    \n",
    "    vrai_longueur = len(phrases[i])\n",
    "    \n",
    "    # Compute Query, Key, and Value matrices\n",
    "    Q = np.dot(phrase_inputs, Wq)\n",
    "    K = np.dot(phrase_inputs, Wk)\n",
    "    V = np.dot(phrase_inputs, Wv)\n",
    "\n",
    "    # Compute attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "     #masquage\n",
    "    scores_masquer = scores.copy()\n",
    "    scores_masquer[:, vrai_longueur:] = -1000000000000\n",
    "    scores_masquer[vrai_longueur:, :] = -1000000000000  \n",
    "    \n",
    "    attention_weights_sans_mask = softmax(scores)\n",
    "    attention_weights_mask = softmax(scores_masquer)\n",
    "    \n",
    "    #pour eviter NAN dans les calcul et les plot ici on force a 0\n",
    "    attention_weights_mask[vrai_longueur:, :] = 0.0\n",
    "      \n",
    "    output_sans_mask = np.dot(attention_weights_sans_mask, V)\n",
    "    output_mask = np.dot(attention_weights_mask, V)\n",
    "    \n",
    "    \n",
    "    phrase_label = phrases[i] + ['padding'] * (max_len - vrai_longueur)\n",
    "\n",
    "\n",
    "    # Plotting the inputs\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.suptitle(f\"Phrase: {' '.join(phrases[i])}\", fontsize=16)\n",
    "    \n",
    "    plt.subplot(2, 4, 1)\n",
    "    sns.heatmap(phrase_inputs, annot=True, cmap='viridis', xticklabels=phrase_label, yticklabels=phrase_label)\n",
    "    plt.title('Inputs')\n",
    "\n",
    "    # Plotting Query (Q) matrix\n",
    "    plt.subplot(2, 4, 2)\n",
    "    sns.heatmap(Q, annot=True, cmap='viridis', xticklabels=['dim1', 'dim2', 'dim3', 'dim4'], yticklabels=phrase_label)\n",
    "    plt.title('Query (Q)')\n",
    "\n",
    "    # Plotting Key (K) matrix\n",
    "    plt.subplot(2, 4, 3)\n",
    "    sns.heatmap(K, annot=True, cmap='viridis', xticklabels=['dim1', 'dim2', 'dim3', 'dim4'], yticklabels=phrase_label)\n",
    "    plt.title('Key (K)')\n",
    "\n",
    "    # Plotting Value (V) matrix\n",
    "    plt.subplot(2, 4, 4)\n",
    "    sns.heatmap(V, annot=True, cmap='viridis', xticklabels=['dim1', 'dim2', 'dim3', 'dim4'], yticklabels=phrase_label)\n",
    "    plt.title('Value (V)')\n",
    "\n",
    "    # Plotting Attention Scores  Sans masque \n",
    "    plt.subplot(2, 4, 5)\n",
    "    sns.heatmap(scores, annot=True, cmap='viridis', xticklabels=phrase_label, yticklabels=phrase_label)\n",
    "    plt.title('Attention Scores Sans masque')\n",
    "\n",
    "    # Plotting Attention Weights Sans masque \n",
    "    plt.subplot(2, 4, 6)\n",
    "    sns.heatmap(attention_weights_sans_mask, annot=True, cmap='viridis', xticklabels=phrase_label, yticklabels=phrase_label)\n",
    "    plt.title('Attention Weights Sans masque')\n",
    "\n",
    "\n",
    "\n",
    "    #Attention Weights AVEC masque\n",
    "    plt.subplot(2, 4, 7)\n",
    "    sns.heatmap(attention_weights_mask, annot=True, cmap='viridis', xticklabels=phrase_label, yticklabels=phrase_label)\n",
    "    plt.title('Attention Scores Sans masque')\n",
    "\n",
    "    #Output AVEC masque\n",
    "    plt.subplot(2, 4, 8)\n",
    "    sns.heatmap(output_mask, annot=True, cmap='viridis', xticklabels=['dim1', 'dim2', 'dim3', 'dim4'], yticklabels=phrase_label)\n",
    "    plt.title('Output AVEC masque')\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Phrase:\", phrases[i])\n",
    "    print(\"Vraie longueur: \",vrai_longueur, \" mots\")\n",
    "    print(\"\\nInputs:\\n\", phrase_inputs)\n",
    "    print(\"\\nQuery (Q):\\n\", Q)\n",
    "    print(\"\\nKey (K):\\n\", K)\n",
    "    print(\"\\nValue (V):\\n\", V)\n",
    "    print(\"\\nAttention Scores SANS masque:\\n\", scores)\n",
    "    print(\"\\nAttention Weights SANS masque:\\n\", attention_weights_sans_mask)\n",
    "\n",
    "    print(\"\\nAttention Weights AVEC masque:\\n\", attention_weights_mask)\n",
    "    print(\"\\nOutput AVEC masque:\\n\", output_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n-------------------------\")\n",
    "print(\"Comparaison Embedding: random et pretrained\")\n",
    "\n",
    "#embeddings aléatoires\n",
    "np.random.seed(42)\n",
    "embeddings_aleatoires = {word: np.random.randn(4) for word in embeddings.keys()}\n",
    "\n",
    "test_phrase = [\"I\", \"love\", \"tennis\"]\n",
    "\n",
    "#avec embedding choisi\n",
    "inputs_perso = np.array([embeddings[mot] for mot in test_phrase])\n",
    "Q_perso = np.dot(inputs_perso, Wq)\n",
    "scores_perso = np.dot(Q_perso, Q_perso.T) / np.sqrt(Q_perso.shape[1])\n",
    "\n",
    "#avec embeddings aleatoires  \n",
    "inputs_aleatoire = np.array([embeddings_aleatoires[mot] for mot in test_phrase])\n",
    "Q_aleatoire = np.dot(inputs_aleatoire, Wq)\n",
    "scores_aleatoire = np.dot(Q_aleatoire, Q_aleatoire.T) / np.sqrt(Q_aleatoire.shape[1])\n",
    "\n",
    "print(\"Scores avec embeddings pretrained:\\n\", scores_perso)\n",
    "print(\"\\nScores avec embeddings ALEATOIRES:\\n\", scores_aleatoire)\n",
    "print(\"\\nLes embeddings aleatoires creent des patterns d'attention differents!\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(scores_perso, annot=True, cmap=\"viridis\")\n",
    "plt.title(\"Attention  Pretrained Embeddings\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.heatmap(scores_aleatoire, annot=True, cmap=\"viridis\")\n",
    "plt.title(\"Attention  Random Embeddings\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09346be9-23ef-4a98-bf5c-479d6a92be98",
   "metadata": {},
   "source": [
    "**REPONSE EXERCICE 2 :**\n",
    "\n",
    "**1.** La phrase plus longue \"I love playing football often with friends\" a été convertie en embeddings 4 bits personnalisé chaque mot est un vecteur comme : \"I\"=[1,0,0,0], \"love\"=[0,1,0,0]. Les matrices Query, Key et Value sont calculées grace à cela donc en multipliant ces embeddings par des poids Wq, Wv qui sont  aléatoire on voit que les embeddings deviennent utilisables pour l'attention\n",
    "\n",
    "**2.** Sans asque, l'attention fait sur tous les tokens et aussi sur le padding gaspille de l'information. Avec masque, l'attention se fait que sur les mots réels donc pour \"I love tennis\" paddé que les 3 premières positions ont des poids non nuls, les 4 positions de padding sont à0.\n",
    "\n",
    "**3.** En comparant mes embeddings structurés avec des embeddings aléatoire on voit une différence qui est que les embeddings structurés produisent des patterns d'attention logique alors que les embeddings aléatoires génèrent des scores qu'on peut pas prédire. C'est pour cela qu'il faut bien les initialiser pour comprendre une phrasee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d817600-6670-4dc3-959a-cdbcc3188051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
